{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93763397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "fpath_end = {\n",
    "    'r': '_R.TIF',\n",
    "    'g': '_G.TIF',\n",
    "    'nir': '_NIR.TIF',\n",
    "    're': '_RE.TIF'\n",
    "}\n",
    "\n",
    "multispectral_bands = ['r', 'g', 'nir', 're']\n",
    "\n",
    "dataset_path = '/media/iittp/new volume/multispectral_validation_set'\n",
    "\n",
    "metadata = {}\n",
    "with open('r_params.json', 'r') as f:\n",
    "    metadata['r'] = json.load(f)[0]\n",
    "with open('g_params.json', 'r') as f:\n",
    "    metadata['g'] = json.load(f)[0]\n",
    "with open('nir_params.json', 'r') as f:\n",
    "    metadata['nir'] = json.load(f)[0]\n",
    "with open('re_params.json', 'r') as f:\n",
    "    metadata['re'] = json.load(f)[0]\n",
    "\n",
    "SAVE_SHIFTED = False\n",
    "SAVE_UNDISTORTED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b2700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_center(path):\n",
    "    cmd = [\"exiftool\", \"-j\", path]\n",
    "    info = json.loads(subprocess.check_output(cmd))[0]\n",
    "\n",
    "    x = float(info.get(\"RelativeOpticalCenterX\", 0))\n",
    "    y = float(info.get(\"RelativeOpticalCenterY\", 0))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def translate_band(img, dx, dy):\n",
    "    M = np.float32([[1, 0, -dx], [0, 1, -dy]])\n",
    "    shifted = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]),\n",
    "                             flags=cv2.INTER_LINEAR,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "    return shifted\n",
    "\n",
    "def undistort_image(image, cam_matrix, distortion_coeffs, crop_percent=0.0):\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(cam_matrix, distortion_coeffs, None, cam_matrix, (w, h), cv2.CV_32FC1)\n",
    "    undistorted_image = cv2.remap(image, map1, map2, cv2.INTER_LINEAR)\n",
    "    \n",
    "    undistorted_image = crop_image(undistorted_image, crop_percent)\n",
    "        \n",
    "    return undistorted_image\n",
    "\n",
    "def crop_image(image, crop_percent):\n",
    "    if crop_percent <= 0:\n",
    "        return image\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    crop_h = int(h * crop_percent)\n",
    "    crop_w = int(w * crop_percent)\n",
    "    cropped_image = image[crop_h:h - crop_h, crop_w:w - crop_w]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def de_vignette_image(image, vignetting_coeffs):\n",
    "    h, w = image.shape[:2]\n",
    "    y_indices, x_indices = np.indices((h, w))\n",
    "    x_center = w / 2\n",
    "    y_center = h / 2\n",
    "    r = np.abs(x_indices - x_center) + np.abs(y_indices - y_center)\n",
    "    \n",
    "    vignetting_mask = np.polyval(vignetting_coeffs[::-1], r)\n",
    "    vignetting_mask = np.clip(vignetting_mask, 0.1, 1.0)\n",
    "    \n",
    "    corrected_image = image.astype(np.float32) / vignetting_mask\n",
    "    corrected_image = np.clip(corrected_image, 0, 65535).astype(np.uint16)\n",
    "    \n",
    "    return corrected_image\n",
    "\n",
    "def to_edges(img):\n",
    "    img = img.astype(np.float32)\n",
    "    img = cv2.GaussianBlur(img, (7, 7), 1.8)\n",
    "    gx = cv2.Sobel(img, cv2.CV_32F, 1, 0)\n",
    "    gy = cv2.Sobel(img, cv2.CV_32F, 0, 1)\n",
    "    return np.abs(gx) + np.abs(gy)\n",
    "\n",
    "def downscale(img, scale=0.5):\n",
    "    h, w = img.shape\n",
    "    return cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def upscale(img, scale=2.0):\n",
    "    h, w = img.shape\n",
    "    return cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def align_image(base_image, input_image, scale=0.5):\n",
    "\n",
    "    base_edges  = to_edges(base_image)\n",
    "    input_edges = to_edges(input_image)\n",
    "    \n",
    "    base_edges  = downscale(base_edges, scale=scale)\n",
    "    input_edges = downscale(input_edges, scale=scale)\n",
    "\n",
    "    base_small  = base_edges / (np.percentile(base_edges, 95)  + 1e-7)\n",
    "    input_small = input_edges / (np.percentile(input_edges, 95) + 1e-7)\n",
    "\n",
    "    warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS |\n",
    "                cv2.TERM_CRITERIA_COUNT, 400, 1e-6)\n",
    "\n",
    "    cc, warp_matrix = cv2.findTransformECC(\n",
    "        base_small,\n",
    "        input_small,\n",
    "        warp_matrix,\n",
    "        cv2.MOTION_TRANSLATION,\n",
    "        criteria\n",
    "    )\n",
    "\n",
    "    aligned = cv2.warpAffine(\n",
    "        input_image,\n",
    "        warp_matrix,\n",
    "        input_image.shape[::-1],\n",
    "        flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP\n",
    "    )\n",
    "\n",
    "    return aligned\n",
    "\n",
    "def save_images(images, output_dir, scale=1.0):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for image in images:\n",
    "        class_dir = os.path.join(output_dir, image['class'])\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "        for band in ['r', 'g', 'nir', 're']:\n",
    "            fpath_out = os.path.join(class_dir, image['fname_base'] + fpath_end[band])\n",
    "            band_image = downscale(image[band], scale=scale)\n",
    "            cv2.imwrite(fpath_out, band_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf5e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 images\n",
      "Loaded 40 images\n",
      "Loaded 60 images\n",
      "Loaded 80 images\n",
      "Loaded 100 images\n",
      "Loaded 120 images\n",
      "Loaded 140 images\n",
      "Loaded 160 images\n",
      "Loaded 180 images\n",
      "Loaded 200 images\n",
      "Loaded 220 images\n",
      "Loaded 240 images\n",
      "Loaded 260 images\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "LOADING_DOWNSAMPLE_SCALE = 0.5\n",
    "\n",
    "for class_name in sorted(os.listdir(dataset_path)):\n",
    "    class_dir = os.path.join(dataset_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # Group files by base filename\n",
    "    samples = defaultdict(dict)\n",
    "    for fname in os.listdir(class_dir):\n",
    "        if not fname.lower().endswith('.tif'):\n",
    "            continue\n",
    "\n",
    "        base, ext = os.path.splitext(fname)\n",
    "        for band in multispectral_bands:\n",
    "            if base.endswith(f\"_{band.upper()}\"):\n",
    "                base_name = base[:-len(f\"_{band}\")]  # e.g., \"img1\"\n",
    "                image = cv2.imread(\n",
    "                    os.path.join(class_dir, fname), cv2.IMREAD_UNCHANGED\n",
    "                )\n",
    "                \n",
    "                samples[base_name][band] = downscale(image, scale=LOADING_DOWNSAMPLE_SCALE)\n",
    "                samples[base_name]['fname_base'] = base_name\n",
    "                samples[base_name]['class'] = class_name\n",
    "\n",
    "    # Convert dict to list\n",
    "    for sample in samples.values():\n",
    "        # Only include samples that have all 4 bands\n",
    "        if all(b in sample for b in multispectral_bands):\n",
    "            images.append(sample)\n",
    "            if len(images) % 20 == 0:\n",
    "                print(f\"Loaded {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1bcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_centers = {}\n",
    "for band in ['r', 'g', 'nir', 're']:\n",
    "    fpath_band = os.path.join(dataset_path, images[0]['class'], images[0]['fname_base'] + fpath_end[band])\n",
    "    x,y = get_relative_center(fpath_band)\n",
    "    relative_centers[band] = (x*LOADING_DOWNSAMPLE_SCALE, y*LOADING_DOWNSAMPLE_SCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccfd629",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_shifted = []\n",
    "max_shift_x, max_shift_y = 0, 0\n",
    "for img_dict in images:\n",
    "    image_shifted = {}\n",
    "    \n",
    "    for band in ['r', 'g', 'nir', 're']:\n",
    "        band_img, (dx, dy) = img_dict[band], relative_centers[band]\n",
    "        band_shifted = translate_band(band_img, dx, dy)\n",
    "        image_shifted[band] = band_shifted\n",
    "        \n",
    "        max_shift_x = max(max_shift_x, abs(dx))\n",
    "        max_shift_y = max(max_shift_y, abs(dy))\n",
    "    \n",
    "    image_shifted['fname_base'] = img_dict['fname_base']\n",
    "    image_shifted['class'] = img_dict['class']\n",
    "    images_shifted.append(image_shifted)\n",
    "\n",
    "max_shift_percent = max(max_shift_x / images_shifted[0]['r'].shape[1], max_shift_y / images_shifted[0]['r'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2130c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "del images  # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77458a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_dataset_dir = 'dataset_shifted'\n",
    "if not os.path.exists(os.path.join(cwd, shifted_dataset_dir)):\n",
    "    os.makedirs(os.path.join(cwd, shifted_dataset_dir))\n",
    "\n",
    "if SAVE_SHIFTED:\n",
    "    for idx, img_shifted in enumerate(images_shifted):\n",
    "        fname_base = img_shifted['fname_base']\n",
    "        \n",
    "        for band in ['r', 'g', 're', 'nir']:\n",
    "            fpath_out = os.path.join(cwd, shifted_dataset_dir, img_shifted['class'], fname_base + fpath_end[band])\n",
    "            if not os.path.exists(os.path.dirname(fpath_out)):\n",
    "                os.makedirs(os.path.dirname(fpath_out))\n",
    "    \n",
    "            cv2.imwrite(fpath_out, img_shifted[band])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce736af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_dataset_dir = 'dataset_undistorted'\n",
    "if not os.path.exists(os.path.join(cwd, undistorted_dataset_dir)):\n",
    "    os.makedirs(os.path.join(cwd, undistorted_dataset_dir))\n",
    "\n",
    "images_undistorted = []\n",
    "for idx, image in enumerate(images_shifted):\n",
    "    image_undistorted = {}\n",
    "    \n",
    "    for band in ['r', 'g', 'nir', 're']:\n",
    "        _, dewarp_data = metadata[band]['DewarpData'].split(';')\n",
    "        fx, fy, cx, cy, k1, k2, p1, p2, k3 = map(float, dewarp_data.split(','))\n",
    "        Cx, Cy = metadata[band]['CalibratedOpticalCenterX'], metadata[band]['CalibratedOpticalCenterY']\n",
    "        vignette_coeffs = list(map(float, metadata[band]['VignettingData'].split(',')))[::-1]\n",
    "        vignette_coeffs.append(1)\n",
    "        \n",
    "        dist_coeffs = np.array([k1, k2, p1, p2, k3])\n",
    "        cam_matrix = np.array([[fx, 0, Cx-cx],\n",
    "                               [0, fy, Cy-cy],\n",
    "                               [0, 0, 1]])\n",
    "        \n",
    "        undistorted_band = undistort_image(image[band], cam_matrix, dist_coeffs, crop_percent=max_shift_percent*1.05)\n",
    "        undistorted_band = de_vignette_image(undistorted_band, vignette_coeffs)\n",
    "        image_undistorted[band] = undistorted_band\n",
    "        \n",
    "    image_undistorted['fname_base'] = image['fname_base']\n",
    "    image_undistorted['class'] = image['class']\n",
    "    images_undistorted.append(image_undistorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833468ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_UNDISTORTED:\n",
    "    num_threads = 16\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        def save_one_image(image, output_dir):\n",
    "            save_images([image], output_dir)\n",
    "        \n",
    "        fpath_out = os.path.join(cwd, undistorted_dataset_dir)\n",
    "        futures = [executor.submit(save_one_image, image, fpath_out) for image in images_undistorted]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "770c1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "del images_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df271cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Alignments: 0, Percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "aligned_dataset_dir = 'dataset_aligned'\n",
    "if not os.path.exists(os.path.join(cwd, aligned_dataset_dir)):\n",
    "    os.makedirs(os.path.join(cwd, aligned_dataset_dir))\n",
    "\n",
    "aligned_images = []\n",
    "num_failed_alignments = 0\n",
    "for idx, image in enumerate(images_undistorted):\n",
    "    image_aligned = {}\n",
    "    base_band = 'nir'\n",
    "    \n",
    "    base_image = image[base_band]\n",
    "    image_aligned[base_band] = crop_image(base_image, 0.05)\n",
    "    \n",
    "    try:\n",
    "        for band in ['g', 'r', 're']:\n",
    "            input_image = image[band]\n",
    "            aligned_band = align_image(base_image, input_image, scale=0.50)\n",
    "            image_aligned[band] = crop_image(aligned_band, 0.05)\n",
    "        \n",
    "        image_aligned['fname_base'] = image['fname_base']\n",
    "        image_aligned['class'] = image['class']\n",
    "        aligned_images.append(image_aligned)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to align image {image['fname_base']} (Class: {image['class']})\")\n",
    "        num_failed_alignments+=1\n",
    "        continue\n",
    "\n",
    "print(f\"Failed Alignments: {num_failed_alignments}, Percentage: {100 * num_failed_alignments / len(aligned_images):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0631897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving aligned images is mandatory\n",
    "num_threads = 16\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    def save_one_image(image, output_dir):\n",
    "        save_images([image], output_dir, scale=1)\n",
    "\n",
    "    fpath_out = os.path.join(cwd, aligned_dataset_dir)\n",
    "    futures = [executor.submit(save_one_image, image, fpath_out) for image in aligned_images]    \n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        future.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
